### Этап №1: Думаем и проектируем
Начнем с железа: было бы неплохо посмотреть на использование ресурсов инфраструктуры, возможно не хватает вычислительных мощностей CPU или утечки памяти в коде.
Железные метрики:
- CPU usage
- Load Average
- Memory usage
- Network traffic
- Disk IO
Теперь посмотрим на метрики бэкэнда, к ним можно отнести:
- общее число запросов
- число ошибок
- RPS
- latency
Частым bottleneck'ом бывает некорректная работа с БД, для нее будем смотреть на следующие метрики:
- количество активных коннектов
- QPS (queries/sec)
- число возвращаемых строк
### Этапы №2-3: Стреляем и анализируем
Для этого этапа будем использовать k6 как предложено.
Для запуска теста необходимо создать двух пользователей в бд:
![Pasted image 20251226025448.png](Pasted%20image%2020251226025448.png)
Сначала посмотрим метрики по железу:
![Pasted image 20251226021415.png](Pasted%20image%2020251226021415.png)
Как можем видеть, в нагрузка в простое скачет до 45%, скорее всего это связано с alt-tabом в виртуалку (у меня на винде не работает докер из-за кривой сборки, поэтому так), волны соответствуют переключению между окнами. Из-за этого дальнейший анализ может быть некорректным.
Рассматриваемые сценарии:
- Сценарий "Шторм" - резкий пик нагрузки (1000 пользователей за 10s)
	Комментим ненужное, запускаем предварительно подождав успокоения CPU Usage
	![Pasted image 20251226021310.png](Pasted%20image%2020251226021310.png)
	![Pasted image 20251226025212.png](Pasted%20image%2020251226025212.png)
	![Pasted image 20251226025235.png](Pasted%20image%2020251226025235.png)
	![Pasted image 20251226025251.png](Pasted%20image%2020251226025251.png)
	99% latency 2 секунды, 30% запросов выдают ошибку, приложение явно не справляется с такой нагрузкой, причем дело не в железе, CPU usage и memory еще имеют запас
- Сценарий "Волна" - плавное нарастание (0 к 500 пользователей за 2 минуты)
	![Pasted image 20251226030049.png](Pasted%20image%2020251226030049.png)
	![Pasted image 20251226030417.png](Pasted%20image%2020251226030417.png)
	![Pasted image 20251226030435.png](Pasted%20image%2020251226030435.png)
	![Pasted image 20251226030450.png](Pasted%20image%2020251226030450.png)
	Благодаря этому тесту с плавным ростом нагрузки мы может определить оптимальную нагрузку и выявить потенциальные bottleneck'и приложения.
	Картина следующая:
	- система начинает деградировать по latency и ошибкам при росте нагрузки
	- упирается не в память и не в сеть
	- Load > 100% при CPU < 50%, значит где-то происходят блокировки
	- нет резкого роста памяти, значит нет утечек
	- использование диска и сети стабильные
	- после 300-350 VUs и 600 rps наблюдается быстрый рост latency, начинаются ошибки
	Скорее всего, проблема в запросах к БД, посмотрим на ее метрики:
	![Pasted image 20251226030500.png](Pasted%20image%2020251226030500.png)
	Как можем видеть, бэкэнд открывает соединения с БД линейно с ростом нагрузки, каждая горутина - свое соединение. рост DB connections -> рост блокировок бэкэнда Поскольку график QPS показывает среднее за 5 минут, то реальный QPS во время тестирования примерно 1300, что очень много для постгреса в докере + видим явное плато на графике.
	Делаем вывод - виновата БД, приложение не масштабируется и не падает, потому что клиенты начинают ждать, часть запросов отваливается, система выживает за счет ошибок. 

	А теперь чуть более формально:
	
	Основным bottleneck является БД: резкий рост количества возвращаемых строк и активных соединений приводит к блокировке goroutines в backend-приложении. Это вызывает рост latency (p99 > 1s), увеличение числа ошибок ~11% и высокий load average при относительно низкой загрузке CPU. Память, сеть и диск не являются ограничивающими факторами.
### Этап №4: Решения
Разделим возможные решения на три типа:
1. Бэкэнд:
	- ограничить число одновременных подключений к БД
	- добавить таймаут на запросы к БД
2.  Архитектура:
	- добавить read-only реплику для БД
	- использовать pgBouncer
3. Observability:
	- ввести метрику на время выполнения запроса
	- ввести метрику на число открытых подключений к БД